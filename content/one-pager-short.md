Teacher evaluation eats administrator time and too often yields low‑signal ratings. **Eval by SwiftScore** helps leaders deliver **evidence‑based, actionable feedback** faster—so teachers **grow**, schools **save time**, and relationships **strengthen**.

Unlike generic AI tools like **ChatGPT** or workflow digitizers like **TeachPoint**, Eval is purpose‑built for teacher growth. It brings the speed and intelligence of AI into a structured, reliable process aligned with instructional frameworks. Where others either generate generic text or simply track compliance, Eval ensures fairness, rigor, and coaching impact.

---

## What we report

We will publish **three composite scores** and **individual metrics**. All metrics use only **evaluation artifacts/history** (notes, write‑ups, component scores, timestamps, evidence citations) and **focus‑group data**.

### 1) TPG — Teacher Professional Growth

> How much teachers are improving in their practice. \
Example: TPG +12 = teachers are improving 12% faster than counterpart.
> 

**How we measure (examples):**

- **Growth effect (g):** Change in rater‑adjusted, framework‑linked latent score (MFRM), aggregated with 95% CIs.
- **Improvement Index:** Percentile‑point translation of g for readability.
- **Evidence→Rating Alignment (ERAI):** Do cited notes justify the rating? (index 0–1).
- **Reliability strength:** G‑coefficient / separation reliability; recommended counts from D‑study.
- **Fairness & drift guardrails:** Rater severity spread and drift checks.
- **Focus‑group growth signal:** Frequency × salience of “I improved” themes.

> Data we use: Component scores over time, rater IDs, framework map, timestamps, component‑tagged evidence, and teacher/principal focus‑group transcripts.
> 

---

### 2) Principal Performance and Efficiency

> How much time principals save and how much more feedback they deliver. \
Example: PPE +20 = principals are 20% more efficient compared to counterpart
> 

**How we will measure:**

- **Authoring time per artifact (min):** Median (P75) from first draft → finalization.
- **Latency to delivered feedback (days):** Observation → delivered write‑up.
- **Evaluation throughput & coverage:** Finalized artifacts/leader/month; **% of teachers** receiving ≥1 observation + feedback per month; streak length.
- **Content richness:** Words/tokens per artifact; **component coverage count**; **evidence density** (citations/100 words); **avg. next steps** per artifact.
- **Coaching production:** Coaching reports/teacher/term; goals set/teacher; **goal specificity rubric** (SMART features present).
- **AI‑assist utilization & impact:** % artifacts using AI‑assist; **Δ authoring time** for assisted vs. non‑assisted items (matched on length/complexity).
- **Pipeline efficiency:** **Note→eval conversion within 7 days**; edits‑to‑final count.
- **Focus‑group usability signal:** Time saved, ability to observe more, reduced admin burden.

> Data we use: Draft/final timestamps, observation timestamps, leader/teacher IDs, text of feedback/coaching notes, component & citation tags, and AI‑assist flags.
> 

---

### 3) RT — Relational Trust

> How strong the relationship feels between principals and teachers. \
Example: RT +15 = teachers and principals report trust and connection that’s 15% higher than counterpart.
> 

**How we measure (examples):**

- **Focus‑group trust index:** Codes for trust, respect, competence, integrity, personal regard.
- **Connection alignment:** Teachers and principals report aligned improvements in connection/communication.
- **Relational Tone Index (NLP):** Affirming language share; **commendation→suggestion ratio**; “we/you” pronoun balance; appreciative openers; constructive hedging.
- **Follow‑through signaling:** % artifacts referencing prior goals/feedback in subsequent notes.
- **Clarity of expectations:** Presence of time‑bound, observable success criteria.
- **Topic alignment (school‑level):** Similarity between teacher‑stated priorities (FG themes) and topics emphasized in principals’ feedback.
- **Sentiment trajectory (optional):** Trend in tone over time (reported cautiously).

> Data we use: Focus‑group transcripts and evaluation/coaching text only.
> 

---

## Study design

- **Where randomization is not possible:**
    - **Matched rollout + Difference‑in‑Differences (DiD):** Stagger adoption; estimate with school & time fixed effects and pre‑trend checks.
    - **Stepped‑Wedge (non‑random):** Deterministic waves where “everyone gets it” in sequence; analyze with period fixed effects and sensitivity tests.
- **Where randomization is possible:** Pair‑matched **cluster trial** (Phase A, 0–6 mo) followed by **stepped‑wedge crossover** (Phase B, 7–18 mo). ITT primary; complier estimates secondary.

Eval is designed to demonstrate not only how it outperforms the traditional manual counterpart, but also how it delivers better outcomes than generic AI outputs or compliance‑oriented edtech platforms.

---

## What we need from schools

- **Data access:** Evaluation artifacts only—notes, final write‑ups, component scores, timestamps, evidence citations; calibration artifacts/logs.
- **Time:** One 45–60‑min onboarding focus groups for administrators with a follow up 1-2 months in; quarterly 45‑min follow‑ups. Optional surveys consistently communicated to all users and teachers.
- **Consent & privacy:** District approval for data artifacts; Focus group/survey consent; FERPA‑aligned sharing.

---

## What you receive

We deliver a **School**, **District**, **Regional**, and **National** brief, plus a **Technical Appendix**.

**Each brief opens with:** **TPG · PPE · RT composite scores**, followed by the full list of sub‑metrics (rolled up at district, regional, national; school‑level in appendices).

Briefs also contextualize results by showing how Eval compares with manual practice, generic AI, and existing platforms—so leaders see not just gains, but the *best way to use AI* for teacher professional development.

---

## Guardrails & quality

- **Cross‑framework linking:** Many‑Facet Rasch (MFRM) to place scores on a common scale.
- **Reliability:** G‑ & D‑studies; inter‑rater checks via vignettes; quarterly calibration.
- **Fairness:** Evidence→Rating Alignment, generosity/leniency audits, drift monitoring.

---

**Let’s help every teacher grow, faster.**

[will@swiftscore.org](mailto:will@swiftscore.org) · [matt@swiftscore.org](mailto:matt@swiftscore.org) · [justin@swiftscore.org](mailto:justin@swiftscore.org)
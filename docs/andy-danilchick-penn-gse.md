<TRANSCRIPT>
Swiftscore - August 18
VIEW RECORDING - 62 mins (No highlights): 

---

0:00 - Andy Danilchick
  How's it going? Thanks. Good. How are you? I don't know what was going wrong with the meeting IDs or whatnot this morning.

0:07 - Will Krasnow (swiftscore)
  Well, thanks for sending a new one. No way. Yeah.

0:11 - Andy Danilchick
  Yeah, I was just sitting there waiting, and then I checked again. I don't know. It messy. Is your brother joining us?

0:16 - Will Krasnow (swiftscore)
  He is joining us. In fact, he is right next to me.

0:23 - Andy Danilchick
  Oh, that's how he's joining. Good to meet How you doing? Good.

0:28 - Will Krasnow (swiftscore)
  Yeah. We were going to – should we do a separate – do you want to hop on a Zoom?  Yeah. And then we're going to – well, we can share audio. Yeah. Right there. Great. Cool. How have things been?

0:46 - Andy Danilchick
  You been good? Yeah. No, it's been – it's a very busy time of year because a lot of programming gets developed and started, and all the educators, they're back in now getting ready for school.

1:00 - Will Krasnow (swiftscore)
  I know. It's exciting, though. Yes. It's cool. Are you on there, Sweet. Cool. I'm pulling up the email that I sent you after our last chat because I know that had a lot of – or it had specific action.  Yeah, I'll get that, too. You left it. Yeah. Let me find it. All right. Got it. So if it's cool with you, I would love to just kind of talk through those things, share about some recent developments that are happening with us that tie into the research.  And then, of course, you know, introduce you and Matt, which is cool. So I'll start that in reverse. So, and as you know, Matt is my brother and co-founder.

2:10 - Andy Danilchick
  Oh, kind of frozen.

2:13 - Will Krasnow (swiftscore)
  You'd find that his is pretty...

2:32 - Andy Danilchick
  Will, Matt, can you hear me? I think we're frozen or something.

2:39 - Will Krasnow (swiftscore)
  Will, can you hear me now? I can hear... How am I?

2:46 - Andy Danilchick
  You're good. Can you say something else? You're a little bit frozen. All right.

3:02 - Will Krasnow (swiftscore)
  Hello, hello.

3:04 - Andy Danilchick
  Can you hear me? I'm hearing you now. I'm going to step out and go back in and help me.

3:44 - Andy Danilchick
  Matt again? Hello? Hello? Matt, can you hear me?

4:11 - Will Krasnow (swiftscore)
  Can you hear me, Andy?

4:12 - Andy Danilchick
  I can.

4:14 - Will Krasnow (swiftscore)
  Okay, so this might be better. I'm going to go camera off to see if this helps.

4:21 - Andy Danilchick
  Yeah, me too. Yeah. Hopefully this will be better.

4:27 - Will Krasnow (swiftscore)
  Matt, can you hear us? to try camera off too? Give me that help. Yep, I can hear you guys.

4:32 - Andy Danilchick
  Sweet.

4:33 - Will Krasnow (swiftscore)
  Awesome. Cool, cool, cool. So Andy, as I was saying, Matt's super technically strong. I think you guys could have a lot of fun together.  He's published some research as well. Two first author papers now.

4:49 - Andy Danilchick
  Nice.

4:50 - Will Krasnow (swiftscore)
  As a junior. And he's at Harvard. And he and I, you know, he's the one who's going to be involved in some of the research that we're doing.

4:58 - Andy Danilchick
  And it's first of all. all we're going to présentation. And And So he's your younger brother?

5:02 - Will Krasnow (swiftscore)
  My younger brother. Oh my goodness. And more accomplished. So somehow figure that one out.

5:09 - Andy Danilchick
  Okay. Well, the older brother sounds pretty humble, but okay.

5:15 - Will Krasnow (swiftscore)
  And so Matt, this is Andy. Andy has kind of followed our journey since pretty early on when we were doing, you know, first iteration teacher grading.  I met him through John, obviously, and he knows his stuff, you know, he knows his stuff about Ed. He knows his stuff about research for sure.  And he runs the Center for Optimal Development at Penn and Penn's Graduate School of Education. So he also has been a teacher, knows these problems firsthand.  And I think, you know, I'm stoked to introduce you both. And I think there's a way in which we could, I think the two of you could collaborate on some really cool.  Research projects to validate SWIFT score and then also enhance the product. So thank you both for being here. Thanks.  Andy, if it's cool with you, I would love to just kind of talk through, we left with those three different aims.

6:23 - Andy Danilchick
  Yeah, I see them now.

6:25 - Will Krasnow (swiftscore)
  Yeah.

6:31 - Andy Danilchick
  Do you want to just hit each one, one by one?

6:33 - Will Krasnow (swiftscore)
  Let's do it. Let's do it. So the first is, yeah, Matt, so it's aligning our tools feedback. I can show you here.  Aligning our tools feedback to research-based practice. So this is, I mean, TLDR, right? Like we're giving feedback. I really want that feedback to be rooted in research as much as possible.  One of the things that Matthew has done really well is like, we can have a lot of control over.  have control over. We We And remember, I showed you, Andy, like that two weeks ago, like the text that comes out.  was pretty powerful. We have a lot of control over what spits out and when. So I mentioned like maybe, you know, you can think of it as like we take point A to B, but we just want to make sure, you know, point B is research-backed.  And so my...

7:24 - Andy Danilchick
  What do you mean by tools feedback? Yeah. So like, like operationalize the tool and the feedback, like in the different forms you imagine it to be.

7:36 - Will Krasnow (swiftscore)
  I think like, there's two ways for, I think about it. One is, is like in terms of like the actual words themselves.  So like, if, you know, I think the example I showed was like a teacher, you know, kind of showing a student or working with a student on how to use a ruler to measure.  And the difference between inches and centimeters.

7:59 - Andy Danilchick
  reminders.

8:00 - Will Krasnow (swiftscore)
  So like, clearly, there's a way to improve the way that the teacher was doing it, right? I want to make sure that the specific direction that our tool recommends is based on pedagogy, which actually works, you know?  And so kind of what we have here is like, we could do a decent job of kind of matching to what old evaluations have done or kind of what AI thinks or like the Danielson rubric.  But even within Danielson or any of these rubrics, like, you know, it just says like, okay, you know, the topic.

8:40 - Andy Danilchick
  You need to do something more than the Danielson rubrics.

8:43 - Will Krasnow (swiftscore)
  It's the Danielson is just a list of stuff.

8:45 - Andy Danilchick
  It's not like a collection of suggestions or anything like that.

8:49 - Will Krasnow (swiftscore)
  So, you know, and it's, I, I, I trying to figure out exactly what this could be, but it could be like, you know, drawing from a repository of like the leading read.  Research across different contexts, right? So it's like, all right, so we know it's a fifth grade class and we know kind of the curriculum or whatever.  And like, but we know that when X happens, the research shows that students learn more, right? And I understand it's not that clear cut.

9:23 - Andy Danilchick
  No, it's not at all. And it's not, it's not something you can't in an objective way, if there is such a thing as objective, you cannot accomplish this goal.  Because like you use the word context, the context shapes situations. And I don't know how your AI can understand the individual students, the teacher's background, the setting.

9:48 - Will Krasnow (swiftscore)
  Right.

9:49 - Andy Danilchick
  You know, but what you could do is some kind of approximation, you know, like based on whatever information you're able to get access to.  You know, it really sounds like an AI problem of what kind of data you can get that can help you do it.  And I'm more interested in thinking about whatever your AI can do to the best of its ability. I think it's kind of like the prompts and the reflective, you know, prompts and things is going to have a lot of value here.  So, like, you might put something out there, but you can have a prompt that helps people evaluate whatever data you get, and you can have, like, universal prompts that are helpful for making meaning of, you know, when people review stuff they've said and done, and they're interacting with AI data, they could make their own conclusions or go back to their own learnings and things, you know, like, let's see if I can think of an example.  It's like, um... You know, you might have questions like, how can, you know, like what students are, may or may not be understanding this, or how, what can be done to reach out to students who are not, you know, moving along with that?  Or what's a different way to present the information to those who might not be learning it? Or what an extension activity that might, you know, deepen the learnings, stuff like, like you can have prompts that encourage the kind of reflection that helps the teacher make more meaning out of what's happened and also think to what they're going to do in the future.  So you think the basis is you're engaging them in a cycle of inquiry on their practice. How do you do that most effectively?  And, you know, the AI data is helpful, but sometimes just getting scaffolding and reflecting is really valuable to, that's what I got.

12:01 - Will Krasnow (swiftscore)
  Andy, are you recommending that those prompts and those questions that we kind of, you're saying like you preload this so that it's almost teaching the AI to think in a certain way, right?  Well, I was thinking, you see, I'm not that well-versed in how to help AIs develop and stuff.

12:20 - Andy Danilchick
  You're right. Those questions could help an AI do that. But I'm thinking more in terms of like the kinds of questions that help educators make more meaning of what they've just done or what texts they're interacting with or their memories of what they did or thinking about their students and things like that.  I know that when, see, I spent a lot of time in my career as a student teacher, facilitator, like a supervisor and supporter, and I spent time also doing evaluations and, you know, just supporting the professional development of adults, like more across.  And, you know, so I'm thinking of, like, engaging them in those kind of processes. Like, a lot of times teachers, they don't have time to do, they just go from thing to thing, so they don't get a chance to, like, dig in and make meaning or make meaning of things together.  Doing this collaboratively is very powerful, too. So, you know, where I'm coming from, I'm not just, I'm not in AI world, so I'm not thinking about how do you make AI's better and more useful.  I see the AI just like any elicitive, like, question or reflective prompt or any kind of artifact that you give a teacher, whether it's a transcript of what they just said, or it's, like, feedback from a survey from kids or whatever.  Like, any of those things prompt reflection and growth and learning for the adult. So, you know, I'm kind of more broadly thinking of that.

13:55 - Will Krasnow (swiftscore)
  And I think I understand what you're saying, and it's about making this feedback. a little little little more. here.  How it was possible to engage the teacher in, you know, reflective thinking or these, is this, you know, let's imagine we found these right prompts and like, would that be research-based?

14:17 - Andy Danilchick
  Is this more or what we would? When you say research-based, we'll be thinking about.

14:21 - Will Krasnow (swiftscore)
  You know, this is actually where my ignorance comes in here. You know, I, presumably there are certain things, Andy, right, like that are like, like, we know based on research, right, that like, one way is better than another, right?

14:48 - Andy Danilchick
  That like, that that is, that is contested space there. Because, and I don't know, like, I don't want to take this on rabbit holes, and I don't want to.  Take you away from things that are most centrally important to what you're doing. But even that there's research is like, you know, sometimes done more generally in education and sometimes more contextual.  Like it has value in a local context rather than more generally. Transfer is a tough problem in education. You know, like what methods are better?  Like it's like, you know, doing phonics or like whole language, you know, whatever, you know. There are certain things that are like effective towards certain ends, right?  You know, like if you want to have more belonging in your classroom, you use the names of your students more.  You greet them when they come into the classroom. You pay attention to the learning environment. And if you want to do good trauma-informed practices, maybe you don't like, you know, some elementary classrooms are overstuffed with so much things in it, it'd be very distracting.  You know, maybe. Maybe you'd have your classroom a little more simply arranged. Maybe you have the light not so bright, you know, in some ways.  But again, like there's some general things and then there's like local knowledge that works. If you're going to have people in a particular classroom, you need to do some kind of mix of the local learning that they're doing, that they know is a good practice in their environment, coupled with some of the more general stuff that's effective, you know.  But even the term best practices is contested in education because some people think if you say best practices, you're saying the best things to do without context.  And that shows a deep misunderstanding of the complexity of education. Places like Teach for America, you know, they're like a big best practices thing.  And they're not really preparing their teachers very well because they need to learn their contexts better. Some educators even use...  I'm not seeing practices as a substitute. That sounds weird to me, but whatever. That's part of their argument.

17:07 - Will Krasnow (swiftscore)
  Do you think, Andy, that there's a way to – again, I understand that there will be significant limitations, and only so far as we could take this, right?  But do you think there's a way where we could identify a couple general things for evaluation, like just bare, bare, bare, basic, you know?  And then, you know, the cool thing about our systems, we're really building it out to try and capture as much context as possible.  So, for instance, you know, we were talking to a district, and they're Slate Valley Unified School District there in Vermont, and they have this really kind of robust, you know, evaluation system, which includes some really good stuff, like teacher agency, the ability to define certain focus goals, and et cetera.  So, let's just Say, you know, one of the teachers at Slate Valley, you know, said, you know, I'm working on, and they worked with their principal, and they said, all right, my focus this year is trauma-informed practice, and I want to make sure my classroom is, you know, taught in such a way that, you know, accommodates for people who have basic significant trauma.  And if that goal was stored and defined within our system, would it be possible for us to have a sort of repository, you know, or having flagged certain contextualized best practice, which then would pair with that evaluation of the teacher?  So, for instance, then the principal goes into the classroom, they observe it, and they note the layout of the room, and then as a result, our AI doesn't just say whatever, you know, AI thinks, which could be good, but specifically...  And thinks and references and says like, oh, well, you know, it's not perfect evidence, but oftentimes in situations such as these, the best thing to do or a good thing to do is lay out your classroom in this way, because there's some studies which support it.

19:18 - Andy Danilchick
  Oh, yeah. There's total information out there about ways to design learning environments for trauma-informed practices. You can get that information out there and you could represent that.  Yes. I was thinking more of like a particular evaluation where there's like a transcript of stuff and what meaning you make out of that.  And then you'd have to teach the AI things like, you know, like some general good trauma-informed practices is like, you know, it relates to some of the belonging things I was saying.  If you're acknowledging students or if you're, you know, sending encouraging, positive, specific feedback. And those are also good teacher practices, too.  So you just have to kind of... Target what you're looking for in a given transcript or for the AI to find that stuff and kind of spit it back or say, when you did this and you did that in this way, you know, when you did this, and there was like a discordant thing, you know, it seemed like this interaction, there was some frustration of the student.  But like, again, you want to shy from the conclusions about what's going on, because no one might know that, you know, you might need to, but if it gets the teacher to reflect on something and think about their next actions, then that's very useful.

20:38 - Will Krasnow (swiftscore)
  So I'm almost wondering if for this first point, right, like, the goal is really, it's almost like a sort of literature review, a sort of organization of, you know, it's like Swiftscore partnered with Andy and PennGSE to collect, you know, and organize some of the  Leading Research Within Teacher Development. And that's not used as an end-all, be-all, but it's one of the pieces of context which goes into our valuations.  And when appropriate, we train our system to draw from that repository and the respective, you know, leading stuff. Because presumably there's some research which is better than others.  We need to make sure we're understanding the research in the right way. We're drawing from it when it's appropriate.  And that work that would need to be done.

21:33 - Andy Danilchick
  Yeah. And there's a ton of stuff in areas like trauma-informed or belonging or cooperative learning or any of the different things you could give specific techniques or something.  Like wanting it. One is the Wilson reading, you know, program approach. Which you could, knowing about what that's like, you could probably match up how effectively you're...  What's happening, you know, in that context of instruction of you have a session or something. Yeah, that stuff's out there.  I mean, we have toolkits that we've made. They're like 70 pages of like research, knowledge, frameworks, tools, and suggestions.  We have one for stress and anxiety, trauma-informed practice, belonging. We have one for well-being, both for students and for teachers.  We're working on a social media literacy toolkit as well. I'm not sure how those would match up because we do that.  Those are research-based. There are a lot of like information out there that is connected to research, and then there's others.  It's just basically people writing about their practice and their beliefs and things like that. So it's a little difficult to sort out some of that.  We can figure that out.

22:51 - Will Krasnow (swiftscore)
  Yeah, cool. Because I'm almost imagining, and tell me, Andy, if you think this would be super hard, but like, I don't, I think it's...  Almost like we would look at the research, right? What's out there. We'd put together, you know, here's the best kind of, here's the really strong, best work for each different context that we can create it in, right?  That there's a relative context and enough body of research to say this is a context which we can provide some insight.

23:25 - Andy Danilchick
  But we organize that all and then, you know, we upload that into Swiftscore.

23:34 - Will Krasnow (swiftscore)
  And then, and I'm imagining this as like, you know, in terms of the offer to school districts is really powerful, right?  Because, you know, you're a superintendent, you say, okay, so one of my strategic goals this year is, I don't know, MTSS instruction.  And so, you know, when I use Swiftscore, I know that my evaluators are being assisted when it's appropriate by...  Pulling and stuff that's a research-backed and with regards to MTSS instruction. Yeah. And I see that as a pretty powerful thing.

24:09 - Andy Danilchick
  Yeah. I mean, the thing that's good in doing this work, when we see it in practice, we'll know if it's any good or if it's working.

24:20 - Will Krasnow (swiftscore)
  You know, like we could just look at it.

24:21 - Andy Danilchick
  right, exactly. Like evaluating its quality is not going to be hard. But there will be challenges in evaluating the research.  So what research do you draw upon? Do you draw upon like anything that's published in certain journals or, you know, that's from dissertations?  Or how do you assess what research might have more value or what's done a little bit better? And then there's like, there's mixed methods research, there's quantitative research, there's qualitative research, and how that all gets sort of worked through.  Could be interesting.

25:04 - Will Krasnow (swiftscore)
  don't know. Right. And that would be the work, right? That would be the challenge that we would try and figure out, right?

25:14 - Andy Danilchick
  Yeah. Yeah, like I can be helpful in us like finding some of that stuff or conceptualizing or just sort of sorting through it.  I mean, it would be, it would require like people like me that have a lot of experience, both in K-12 and research to sort of know what we're looking at.  You know, like the danger is you take it all in. How do you rate the different things? And do you just pull something out from someone's random dissertation?  That's a piece of crap. But maybe even it's a piece of crap. Maybe the thing is useful anyway. I don't know.  It's like when you get AI responses from search engines and they're saying some. Stuff, and you're not sure where it's coming from, where it's sourced, and it feels like it's right, or it might feel like it's a little, how do we really know?

26:11 - Will Krasnow (swiftscore)
  Cool. All right.

26:13 - Andy Danilchick
  I like it.

26:14 - Will Krasnow (swiftscore)
  I like it. Let's move on to the next one and then recircle back to this one in a second.

26:21 - Andy Danilchick
  I love that word, research-based.

26:24 - Will Krasnow (swiftscore)
  Yeah, yeah. Yeah, so guidelines for implementation and use, right? Like, I don't know exactly how we can get at this one.

26:34 - Andy Danilchick
  Well, we started talking, we started talking about it in one, you know, like, where do we, well, I have to understand, what do you mean by research-based guidelines for implementation?  Like, how you construct the AI, or how you construct the process from people using the AI, or what do you think?

26:53 - Will Krasnow (swiftscore)
  I think it's, I think it's the second, you know, like, if there's a way that we can say, hey, you know, Swift.  Score Andy Penn came together and created this kind of commitment to excellence little booklet, right, that everyone gets when they purchase Swiftscore, you know, saying, hey, this is probably how you should use the tool.  This is how you shouldn't, you know, and drawn things like, I'm sure there's research out there that's something to do with, like, increased connection between teacher and principal, you know, leads to better results, right?  So then, you know, you would say something along the lines of, you know, you want to continue to meet with teachers and you recommend doing that.  I actually think it's twofold. It's really that, like, we on the internal Swiftscore team are doing these kind of research-backed developments to develop the AI process to be most effective for when principals are inputting context information and then helping guide them to give the right information that the AI needs.  To give that best experience.

28:04 - Andy Danilchick
  And where like these guidelines are like great practices for how you optimize your learning from this or and you get at the heart of a really good professional growth kinds of dynamics or things like that?

28:18 - Will Krasnow (swiftscore)
  Exactly. So then this is like where I imagine we would want to collect research about processes of more of the meta thinking, right?  So not necessarily here's what, you know, is the best type of instruction, but here's the best process for instructional development.

28:35 - Andy Danilchick
  So in a way, you're not just building, quote unquote, agency or good professional growth practices for an individual. You're doing it for groups, for leader, worker dynamics, and also for a system, so to speak.

28:52 - Will Krasnow (swiftscore)
  Right. I think, Andy, that kind of addresses your initial kind of concern of like, yeah, there's not necessarily just good practices for everyone or.

29:00 - Andy Danilchick
  Yeah. How do you determine what's the best?

29:03 - Will Krasnow (swiftscore)
  But we can kind of help guide the principals or the administrators to, because they're the ones who, when they understand their own teachers, we want to help guide them to best help them in the way that they see fit.

29:16 - Andy Danilchick
  So we're describing the guidance. Right. And you could center SISC score as like a comprehensive approach towards deep transformational change in a system, too, if you wanted to, if you wanted to promise something big.  But you have to deliver like, like information and guidance and support in those areas. So I think a big, if we were to create like buckets in this, one bucket is sort of encouraging, well, there's one about like leveraging reflection and inquiry for growth, right?  And there's a whole bunch of stuff around there. But then there's related areas such as like. It's agency as a teacher to kind of monitor your own growth.  It's like a Zen diagram with the reflection thing. But the idea of fostering agency is very good. And you could talk about it as individual agency and also collective agency because another big bucket is collaboration for professional growth.  And then another area, and these all overlap, we can make some kind of dynamic geometrical design about this, some kind of ecological systems theory of teacher professional growth and learning and transformation or whatever.  But there could be, you know, like, what's the right word? Like leader-teacher relationships or like role of leader in encouraging professional growth.  And under that is the stuff on, you know, being in dialogue or how to be supportive or how to encourage the agent.  And reflection as a, as the leaders, not necessarily, some folks think they do evaluations, you're supposed to tell people who they are and what they're doing.  And that's usually discordant and not very helpful. But if they're engaging in an ongoing process, you know, like, like just in the continuous improvement stuff in Carnegie, you know, like, like, that's a good frame to look at this in a way.  And there may be some other buckets, you know, like, you know, how do you could, I mean, there could, I mean, you want to look at teacher practices, as the center of all of this is where the growth can happen.  But you might even have like an additional, additional kind of thing about like, you know, leader practices, like the building leader, like a principal, assistant principal to do the evaluations.  And you may also have something that engages superintendents or center offices and like. A review of the system and how that encourages kind of things and all of that.  Because, you know, do you have walkthroughs in your school? How are those walkthroughs set up? Do the teachers have an opportunity to, like, give information about what they want to know in a walkthrough?  So the walkthroughs are, they feel like they own the process and they welcome people to come. Or is it more center office designed, what's in a walkthrough and how people have to either perform on that particular thing to be okay or...  or engage in real genuine learning or something? I don't know.

32:36 - Will Krasnow (swiftscore)
  Because also what this can serve us, too, is it stands as more than just tech, right? It positions us as really system thought leaders and, you know, and a resource for making systemized change.  And I even imagine this resource not only being given to, you know, people as they... by the tool, but potentially even people that don't, you know, and have this be like, hey, like, this is, is, are you, are you doing these things within your system at your district?  And odds are you're probably not doing all of them. And also, if you want assistance in implementing these things, there's this great tool, Swiftscore, which can help you do more walkthroughs and help you do this.

33:25 - Andy Danilchick
  And this is how you would use the tool in that way. Yeah. Yeah. I mean, you could, what you're talking about doesn't have to be limited to a school.  could do it in any organization, you know, to optimize what they do. I mean, you, you don't, you don't want to do that because you need to like do it in a particular area to build success with.  But at some point, if you wanted to develop, this is like the, the stuff of like how to have successful life and how to optimize.  I mean, this is like, I'm, I'm kind of obsessed with optimal development as a thing. I mean, that's the name of our project, but it really is an engine for that.  But the thing is, you know, with the tech, when tech stuff really heated up around the early 2000s and there was a lot more, you know, implementation of tech, there's a lot of talk about, you know, value-added approaches to integrating technology into schools and, you the keywords being value-added and integrating.  So you weren't using the tech to drive instruction or to replace something as much as you were using it to add value, to be a tool, to be a data source, to leverage something, you know, and it was done in very intentional ways, not as the thing that, you know, you want, sometimes people like dumb down themselves when they're interacting with technology and let the tech do something for them.  Like, like, this kind of stuff, you want the tech to engage them, to, like, level them up with their cognitive focus and, you know, engagement, you know.  I don't know.

35:02 - Will Krasnow (swiftscore)
  Right. A hundred percent. And it makes our company that much more meaningful, right?

35:06 - Andy Danilchick
  Yeah.

35:07 - Will Krasnow (swiftscore)
  And committed to something much more than just like, quote unquote, efficiency. Okay.

35:13 - Andy Danilchick
  Amazing. Yes.

35:14 - Will Krasnow (swiftscore)
  I think we're aligned about that one, which would be pretty cool and awesome. And then the third is really, you know, making sure that this tool is like good in practice.  And Matt, I'll let you actually talk a little bit about kind of some of the work that we're going to start doing.  So as we're selling this, you know, across the country, we've actually partnered with these two guys at a group called Rocket PD.  And they do like, they have a lot of connections across the country and they do professional development. They sell like courses to like from videos of like former educators to educators that are currently practicing.  And so one of things we're doing. Doing is we're actually bringing together regional and inter-regional kind of comparisons of how people are going to use the tool.  And we're kind of lining this up as this kind of whole cohort experience where we're going to different districts in Massachusetts or Ohio and we're saying, hey, you know, we're doing this kind of program where if you are interested in sponsoring a couple of schools to use our tool and we show the tool.  And then they usually are pretty excited. They want to use it, but they kind of want to test it out.  Then, you know, you could pay to be part of the study or and we're going to evaluate these. We're going to give access to these people.  You're going to anecdotally see if it's valuable, but then we're also going to step in, define some key metrics in the beginning and see if these have been achieved within 6, 12, 18 months.  And actually, the beauty of only having a few schools take part, which is usually what these districts want, is that we could look at counterparts.  Art schools as well within the district. And so Matt has been thinking about really how we can, and this is something we're all putting, we're putting together now, thinking about how we can really lean into this as some pretty nice research into is our tool doing the things that it's supposed to be doing?  And I think that there's some alignment here between Matt kind of going out and doing the work, and Andy, you kind of looking at some of Matt's questions and ensuring that, like, this is aligned with the right way to think about these things, what's the general research, you know, We can design it together.  And then Matt to go out and really kind of delve deep into this, what could be a really, really fascinating study, and it's looking up, sizing up to be, we could probably get, you know, 100 schools who are using the tool, and potentially not.  Andre, in terms of counterparts.

38:03 - Andy Danilchick
  Yeah. I mean, certainly we could centrally design, we could do the research design or the program evaluation, but there's, how much do you know about the difference between program evaluation and like academic study?  How do you understand both of those things?

38:28 - Will Krasnow (swiftscore)
  From my understanding, program evaluation is, is not only just like the design of it, but also understanding how this, these results impact, you know, the action.  So it's a little farther than academic.

38:46 - Andy Danilchick
  Yeah. mean, that, that certainly is a good distinction. I mean, when you think about program evaluation, it's sort of an, it's a research approach.  It's kind of ongoing and not necessarily situated in the. good You know, if you're doing a study, especially if you're doing a study that's quantitative, you set up your study, and then you implement it as you designed it.  You know, very rarely do quant studies get changed, or sometimes you might do additional aspects to it. But, like, with the program evaluation, you might have, you might be getting feedback in real time, or data in real time, and you could share that data with the people on site right away, and you could make changes and adjustments right away, internally, externally, to any of the stakeholders involved in it.  So it's like, you know, when you're doing a study, you typically do it, and you let people know what you've done at the end, you know?  There's some qualitative studies that's a little bit more interactive, but, like, I think you want to have elements of both.  Like, so, you want to be able... To like get your input, set up your things where people are, they're engaging with it and they're maybe answering some basic questions so you know what their experience, like a user experience, you know what they found valuable, what they found not so valuable or was annoying or they might make a suggestion on how to improve the process or, and they might share specific things like from this experience I was able to do this or I was able to make these kinds of gains.  And if you're able to, if people are engaged in the process over multiple intervals, you can then do comparison stuff of how they're growing with it or what they're doing or how something that they started gets extended.  You know, sometimes people are engaged in evaluation multiple times. Your tool can certainly do that with people if districts want to do that.  You know, a study is typically something where people give a waiver to something, you know, and, and... And you tell them what the study is and what their rights are, the ethics involved, and then you kind of do it.  And then at the end, you give some kind of report. Sometimes you publish it or something like that. You want to be thinking of both program evaluation and study things at the same time and how that best supports the growth of your company and improves the product and things like that.

41:26 - Will Krasnow (swiftscore)
  Yeah, totally with you on that. And that's actually kind of what I baked into this kind of initial proposal that I've drafted, where we kind of have a pair-matched, parallel cluster, like RCT, for the first six months, where we have the kind of counterparts and then the ones treated with eval, the program.  And kind of not, and just, you know, consistent surveying and monitoring of their data. And then really in that kind of seven...  To 18-month range, kind of a stepped wedge rollout to kind of see where rolling out new features continuously to existing users and then trying to get to a point where all the schools are receiving eval.  As the product develops, they're all receiving eval and then we can do comparisons within schools, et cetera.

42:26 - Andy Danilchick
  I mean, it would be good in any kind of research or product evaluation design. You'd want to do individual interviews and some focus groups, too.  I that would really, you know, I mean, you can have like surveys or people as part of them doing stuff, they have to answer some stuff and that's useful.  But, you know, we look at that data and then we bring it to like entities and focus groups and it further contextualizes it and further develops it.  You can get your testimonials out of that stuff, too. delays. can Good B Bye.

43:01 - Will Krasnow (swiftscore)
  Yeah, and all that goes right into the product, right? Because then we get all that information about their pain points, what's working.  And then, you know, if this was a pure study, we wouldn't really be able to help them with that information that we know.

43:15 - Andy Danilchick
  But, you know, I mean, education, education is so complex, you know, like, quote, unquote, pure studies, like control and experimental groups.  And, you know, I don't know if you want to do that, you know, because you, I mean, I guess you could find a way to compare people that don't get your product and they're lesser than those that do.  But, you know, you could show growth within the people that do it, you know, you don't have to spend all this money on people that aren't doing it.  I don't know. And we could, we could talk about that a little bit more. Like, it really depends what your goals are and what you want to accomplish.  Yeah, I also think there's value. And building in to the experience itself, like that kind of feedback, I don't know if feedback is the right word, or findings or something.  So if you're building in people that are doing, say there's like a cohort of a certain people going through swiftscore to school, you can break them up into small groups to collaboratively talk with each other where they share about their experiences.  You know, I did this, this is what I learned, this is what I'm trying to do. And you're going to have sessions where it's an extension session, where it's like someone has their individual experience, maybe they talk to a leader a little bit.  But if they're also talking to colleagues, you're gathering all this data about their experience, but you're also enhancing their learning and application and growth from it as well.  So there may be opportunities to build it into the thing, you know.

44:59 - Will Krasnow (swiftscore)
  Absolutely.

45:01 - Andy Danilchick
  Teachers get annoyed at doing extra surveys or extra other things. They generally enjoy doing interviews and focus groups and get them to agree to that.  But if everything's sort of built in as part of the process of the thing, they're not going to have any kind of annoyance at all.  They're probably going to appreciate the engagement of that.

45:20 - Will Krasnow (swiftscore)
  So yeah, like the idea of, you know, this can be more like, we don't have to go into the rigor.  rigorous RCT or any of that yet. You know what I mean? Like, let's use it. At this point in the stage of the company, it's so important to internalize, share our findings pretty immediately and use that to improve user experience.  And then by the time this kind of study is wrapped up, I would think that, you know, we would, the product would be in such a great place.  It would make sense to devote a ton of resources into something that's like, that's locked where we can't. Change it so much.

46:03 - Andy Danilchick
  And answer it, RCT and all that kind of stuff. Yeah, there might be, that might be useful to set that up, you know, like you could even do something within a school where you randomize, you do this with some of the teachers and you don't do it with others, you know.  Right. And yes, you can do like, I don't know, it's kind of a game of deciding what are the comparison metrics you're going to do, but you can even do something like, you can even have qualitative interviews and people that do it and people that don't, and you could show some differences in practices or differences in reflection, you know, and make a case for, you know, one of the other, you know, like.  Well, think what, what you'll, what you would, if you did a study like that, what you'd show is the importance of, how do you say, scaffolds or structures to help optimize.  Growth and improvement. Like, you could show that there's heightened reflection that happens with folks that do swiftscore. You might even be able to make a case for the importance of this with new teachers or teachers in the first.  Typically, you think of novice teachers in their first five years, and school districts usually set up career induction programs.  So there might be a way of integrating this with induction as a way to speed up folks going through the novice stage.  There's so much complexity in teaching. Teachers in their first year are, like, super overwhelmed with all the sensory stuff.  They have a hard time, you know, observing in their field of practice, like, making sense of the overwhelming data that comes their way.  And they might struggle, especially some areas that might be particularly skilled, and others that might struggle. Like, I'll give you my example.  I was really good at intuition and know. Students and paying attention to them and building relationships. I was terrible at pacing as a teacher, you know, I would spend too long on the Civil War or something like that, you know, but you, you can find ways to help teachers identify some of those areas they're not, they're not as great at, or even have them reflect on the areas they seem to be pretty strong on and how they can leverage it out for future groups.  I don't know, I think there are particular applications for novice teachers in this that can be especially helpful to optimize growth, and you can write about that, and that could be, that could be impressive in the field.  But here's some, one of my passions is, what do we, we have all these things to try to help new teachers, and it's messy, but, you know, hopefully they get through it and they don't leave, you know, a ton of them leave.  But what about the veteran teachers, you know, what do we do to support them, and we have to do less stuff at them, but more stuff that gives them spaces to engage.  With each other, or even more identically with themselves. There is a, I'm forgetting the author for now, I'll remember it later today, but there was this guy that did a study of teachers going through different stages, and, you know, like the novice stage is the first five years, and then there's like another five years where you're kind of like, not an expert yet, and then you get to like your expert stages and whatever.  But a lot of teachers, they get to proficiency, that's like the stage of like year five, generally, you do go through proficiency from year five to 10, or five to nine.  A lot of folks, they get to proficiency, and they stop growing, and they stop being interested in growing, and they just do the work, you know, the kind of system kind of supports that.  But the teachers that are reflective, intentional, agentic, they get to expertise quicker, and one of the things you find out, like I could talk to my personal experience, when I got to year nine and 10.  Thank And I was able to pretty much do everything I wanted to do as a teacher, I can imagine, which I couldn't do all 100% earlier.  I finally got to that stage. And then I saw all this other opportunity for me to grow. Like, I didn't see that opportunity to grow when I was a younger teacher.  too busy or too overwhelmed. But when you get to that beginning expertise, the growth pathways become more clearly illuminated and inspiring in a way.  So there's, knowing how teachers go through their growth and the development cycle of a professional, especially within teaching, Swift Score can have an opportunity to impact with different stages in different ways.  And might need to be constructed, maybe not constructed, but maybe engaged differently by an expert teacher or for a major.  You know, because maybe with a new teacher, you might break down how they're reflecting and what they do into like little components, like focus on this area, do this.  Whereas an... Expert teacher, they might just be way more complex about how they engage with it or what they want to do with it.  You know, whereas with a younger teacher, you don't want them being thinking too expansive, but you want them a little bit more concrete, if that makes sense.

51:18 - Will Krasnow (swiftscore)
  Yeah, yeah, it does. It does. It's so interesting. I mean, this is what I think makes this work so exciting is that, you know, there's a way to get its context, right?  It's the different stages of the teacher. It's the different environment. the different things they need to grow. And then how can we, yeah, make this into our tool, you know, make sure it's in the system of implementation, and then ensure that it's really doing the things it's supposed to be doing through reflective research, right?

51:56 - Andy Danilchick
  If you want to really add a research element that I don't think you've thought about yet. You can have the people doing this be almost like practitioner researchers, that they're researching their own practice.  And they're, I mean, there's action research, practitioner inquiry, practitioner research, there's different terms for it. But, you know, if you're engaging people to study their work, like, have their own findings, so to speak, and next places to research or next places for action or input, like that.  That is not a bad way to engage people in this. I think you got to be careful, though, like, when, for people to identify as a practitioner researcher or an action researcher, that's like next level stuff.  And most people don't do that. You know, like, I ran a program for 10 years at GSE called Action Research Groups, and we would bring in different educators, teachers, leaders, counselors, whatever.  And we'd meet once a month over the food at Penn and support. Each other with action research projects, we would sometimes write our stuff up and publish them in journals, present at conferences.  You you could have an aspect like, I don't know, like the all-stars or people that want to go next level with this, you know.  Or you could integrate a little bit of a professionalization thing where, you know, seeing teachers as practitioner researchers. Like our Swiftscore thing is designed to help you know what you're doing well and know where you can keep going and support your own, you know, inquiry, creative growth, and professional development.  I don't know, all of those things.

53:44 - Will Krasnow (swiftscore)
  It's so interesting. It's so interesting. That's something I got to think about because that could be pretty cool and a great way to drive engagement.

53:53 - Andy Danilchick
  Yeah.

53:54 - Will Krasnow (swiftscore)
  Which is fascinating. Cool. So, it seems like we're pretty aligned. I... love all of these directions. I want to move forward with all three, potentially even four.  have to think about that one. You know, my, where do we go from here, Andy? Like, my thought is, you know, is there a way, as I mentioned, you know, Matt can handle a lot of this legwork.  So is there a way we could kind of bring you, you know, Penn's Center for Optimal Development into this, just to make sure that we're asking the right question and kind of as an almost an overseer, like, you know, maybe this is the wrong analogy because I'm not in research.  So Andy, Matt, you can tell me if I'm wrong, but almost like kind of be Matt's PI in a sense of like, just, you know, grounding this stuff in what is research, making sure we're thinking about things the right way, and then having him just go out and, and, and.  Crush it.

55:04 - Andy Danilchick
  Um, so, and I'm, like, I think brain is, I can, so there's, you know, kind of, you desire to maintain a little bit of your, your value.  And, um, it can also help you craft your narrative, or what your, what your thing does, and how it can make change and be helpful, like, and I can speak for, you know, to superintendents, or schools, or, you know, whatever stakeholders to help, um, increase, like, the power and effectiveness of how your, your message gets out, and how people see it, um, and our project can be...  We can become partners, you know, in the sense that, you know, we do research and studies for you, with you, and we can publish some of that stuff, too.  So it can happen on multiple levels. So I don't know. I guess the thing to think through is, like, what are the different areas that we imagine that I can do work with you and for you?  And how does our specific project and pen, as a partner, like, interact with you or bring value to the whole thing?  So, you know, I would suggest, depending on your funding, we could do it where you pay me, like, a monthly thing, you know, when we start out with something and just see how the workload aligns to that.  But I think there's a lot of concurrent areas where we can do some things.

57:03 - Will Krasnow (swiftscore)
  Cool. Cool. I am very excited. I have a couple, I think my thought would be, we should probably, and Matt, curious what you think, we should probably think about everything Andy just said, think about what we kind of just ideated here a little bit together, and then flesh out for ourselves what we think that looks in, and then, you know, articulate this is what Matt does, and this is what, this is what Andy does, and then go from there, and as I mentioned earlier, right, like, we are, we have money coming in, it's not, it's not, like, absorbed in amounts, so there is something we could, we could do from the get-go, and then if there's a way we can, you know, make this into a thing where it reflects our early stage, but then also ensures larger kind of studies and stuff, and, and, and compensating.  And when we grow, we'll kind of think about how we can make that into this whole work, because we really do want to work with you on We can also apply for grants together.  Right.

58:13 - Andy Danilchick
  That could be cool. I don't know if you could apply for something like a Spencer grant, like a research thing, choosing universities, but we could do it together, you know, things like that.  And there might be grants, tech grants, that having a university partnership could enhance your ability to get them as well, you know.

58:37 - Will Krasnow (swiftscore)
  I think a hundred percent. And Andy, I'm just thinking out loud here, is that the kind of thing that would be meaningful to kind of go through you there, is kind of through PEN, and for me to kind of be in your group and kind of go through the PEN route for grants?

59:00 - Andy Danilchick
  Yeah. You know, and those you go through us and plan to do it. But there's other things that are like, you know, the business competitions you have and other kinds of stuff.  And you might be able to apply to those more collaboratively. I don't know. I think we look at, we see where grant comes and it tells us the strategy of how we go for it.  And some foundations have like different kinds of grants, you know, like, for example, Spencer has straight up research grants, but they also have research to practice partnerships.  So we might do a grant with a bunch of school districts, and our grants get supported. We enhance our grant by getting letters to support from the superintendents and local people and things like that.  I mean, we have experience doing grants. You know, we're lucky enough to get this, just this summer, we have a million dollars of grants that came in for, for the, from the opioid trust element, which is exciting.  But yeah, we can figure it out. Yeah. I mean, generally, like, you want to do, you can't count on grants for your thing.  Like, for me, revenue is what keeps me going, but grants can really enhance you or help you do some things, too.

1:00:24 - Will Krasnow (swiftscore)
  Yeah. I, I'm with you, Andy. I think this is all really, really exciting. Let's, how about Matt and I, let Matt and I think about it, we'll flesh this out.

1:00:36 - Andy Danilchick
  Mm-hmm.

1:00:39 - Will Krasnow (swiftscore)
  And we'll get back to you with, specifically, what, how we can work together, and, and what opportunities we can tap into, what that, what that workload actually entails.

1:00:50 - Andy Danilchick
  Yeah.

1:00:52 - Will Krasnow (swiftscore)
  And, and then we can, we can go from there, whether that's set up another meeting or, or whatever it is.  Yeah. But I, I'm super stoked by this, and I think there's a way that we can, we could really make this sing, and I think that'd be really cool.

1:01:05 - Andy Danilchick
  Really, really Sure. Just reach out to Mary with times you're available and we can meet next and cash it out.

1:01:11 - Will Krasnow (swiftscore)
  Sweet. I really appreciate the time. it was a, it's always a pleasure. So many ideas moving and I'm excited that it feels like we're approaching something, something more precise each time, you know?

1:01:23 - Andy Danilchick
  Well, every time I talk to you, I believe in you and what you're doing more and more, which is good.

1:01:29 - Will Krasnow (swiftscore)
  Thanks, Ian.

1:01:29 - Andy Danilchick
  I appreciate it. aligning.

1:01:31 - Will Krasnow (swiftscore)
  Yeah, I love it. Okay, cool. Have a great day, man. You too. Take care. Bye. right. Thanks, Andy. See you.  Nice to meet you. Good to meet you. Yep.

</TRANSCRIPT>